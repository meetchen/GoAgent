import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict, Optional

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()


class GoAgentLLM:
    """
    å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯ã€‚
    ç”¨äºè°ƒç”¨ä»»ä½•å…¼å®¹OpenAIæ¥å£çš„æœåŠ¡ï¼Œå¹¶é»˜è®¤ä½¿ç”¨æµå¼å“åº”ã€‚
    """
    def __init__(self, model: str = None, api_key: str = None, base_url: str = None, timeout: int = None):
        """
        åˆå§‹åŒ–å®¢æˆ·ç«¯ã€‚ä¼˜å…ˆä½¿ç”¨ä¼ å…¥å‚æ•°ï¼Œå¦‚æœæœªæä¾›ï¼Œåˆ™ä»ç¯å¢ƒå˜é‡åŠ è½½ã€‚
        
        Args:
            model: æ¨¡å‹IDï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡LLM_MODEL_IDè·å–
            api_key: APIå¯†é’¥ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡LLM_API_KEYè·å–
            base_url: æœåŠ¡åœ°å€ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡LLM_BASE_URLè·å–
            timeout: è¶…æ—¶æ—¶é—´(ç§’)ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡LLM_TIMEOUTè·å–ï¼Œè‹¥æœªè®¾ç½®åˆ™ä¸º60ç§’
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        api_key = api_key or os.getenv("LLM_API_KEY")
        base_url = base_url or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        if not all([self.model, api_key, base_url]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")

        self.client = OpenAI(api_key=api_key, base_url=base_url, timeout=timeout)

    def invoke(self, messages: List[Dict[str, str]], temperature: float = 0, **kwargs) -> Optional[str]:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆå›å¤ï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        
        Args:
            messages: å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›å¤çš„éšæœºæ€§ï¼Œé»˜è®¤ä¸º0(æœ€ç¡®å®šæ€§)
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼Œå½“å‰ä¼šè¢«å¿½ç•¥ï¼‰
            
        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¦‚å‡ºé”™åˆ™è¿”å›None
        """
        print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            
            # å¤„ç†æµå¼å“åº”
            print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:")
            collected_content = []
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)
            print()  # åœ¨æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ
            return "".join(collected_content)

        except Exception as e:
            print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None
    
    def stream_invoke(self, messages: List[Dict[str, str]], temperature: float = 0, **kwargs):
        """
        æµå¼è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹ï¼Œé€å—è¿”å›å“åº”å†…å®¹ï¼ˆç”Ÿæˆå™¨ï¼‰ã€‚
        
        Args:
            messages: å¯¹è¯å†å²æ¶ˆæ¯åˆ—è¡¨
            temperature: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶å›å¤çš„éšæœºæ€§ï¼Œé»˜è®¤ä¸º0(æœ€ç¡®å®šæ€§)
            **kwargs: å…¶ä»–å‚æ•°ï¼ˆä¸ºäº†å…¼å®¹æ€§ï¼Œå½“å‰ä¼šè¢«å¿½ç•¥ï¼‰
            
        Yields:
            æ¯æ¬¡ç”Ÿæˆçš„æ–‡æœ¬å—
        """
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                if content:
                    yield content
                    
        except Exception as e:
            print(f"âŒ æµå¼è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            yield ""
